Create a custom TransformerEncoderLayer that has the same functionality as the pytorch-version but stores the attention matrices and their corresponding gradients.